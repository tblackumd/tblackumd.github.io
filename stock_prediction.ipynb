{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import requests\n",
    "from datetime import datetime, date, time, timezone\n",
    "client_id = '' #your client id would go here\n",
    "apikey = '' #your API key goes here\n",
    "callback_url = '' #your callback URL goes here.\n",
    "    \n",
    "#TD Ameritrade API requests come with a datetime that is in the form of milliseconds from epoch.\n",
    "#It's likely we will want to be able to convert standard datetimes into this form, if only to be able to convert dates to epochs\n",
    "#so that we can request the appropriate date ranges.\n",
    "def unix_time_millis(mydt):\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    return (mydt - epoch).total_seconds() * 1000.0 \n",
    "\n",
    "print(datetime.now())\n",
    "print(unix_time_millis(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, I have compiled a CSV file of nasdaq traded stocks.  It is from this that I will make a list of the stocks that I want\n",
    "#to request historical data for.\n",
    "import csv \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "file_name = \"nasdaqtraded.csv\"\n",
    "with open(file_name, 'r') as f: \n",
    "    reader = csv.reader(f)\n",
    "    data = list(list(rec) for rec in csv.reader(f, delimiter=',')) #reads csv into a list of lists\n",
    "\n",
    "stocks = []\n",
    "for l in data:\n",
    "    symb = l[1]\n",
    "    stocks.append(symb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load,dump\n",
    "#We can form our payload for our request ahead of time based on what type of data we are trying to get.\n",
    "payload = {'apikey':apikey,\n",
    "           'frequencyType':'minute',\n",
    "           'frequency':'30',\n",
    "           'startDate':'',\n",
    "           'needExtendedHoursData':'true'}\n",
    "\n",
    "for symbol in stocks:\n",
    "    if path.exists(\"stonks/\"+symbol+\"30\"):\n",
    "        continue\n",
    "    stocks30 = [[],[],[],[],[],[]]\n",
    "    \n",
    "    endpoint = r\"https://api.tdameritrade.com/v1/marketdata/{}/pricehistory\".format(symbol)\n",
    "\n",
    "    #grab the 30min candles for the stock.  Set the date range to the appropriate epochs.  To cover\n",
    "    #larger time periods, we must make multiple requests.\n",
    "    for t in range(1556179200000,1578664377362,4000000000):\n",
    "        sval = str(t)\n",
    "        payload1['startDate'] = sval\n",
    "        content = requests.get(url=endpoint,params=payload1)\n",
    "        data = content.json()       \n",
    "        \n",
    "        for entry in data['candles']:\n",
    "            myvolume = entry['volume']\n",
    "            mydt = entry['datetime']\n",
    "            myclose = entry['close']\n",
    "            myopen = entry['open']\n",
    "            myhigh = entry['high']\n",
    "            mylow = entry['low']\n",
    "            stocks30[0].append(int(mydt))\n",
    "            stocks30[1].append(float(myvolume))\n",
    "            stocks30[2].append(float(myopen))\n",
    "            stocks30[3].append(float(myclose))\n",
    "            stocks30[4].append(float(myhigh))\n",
    "            stocks30[5].append(float(mylow))\n",
    "\n",
    "    #We dump the historical data of each requested stock into it's own file for use later.\n",
    "    dump(stocks30, \"stonks/\"+symbol+'30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catalysts such as news, earning reports, dividends, etc are important for stock prediction.\n",
    "#I manually scraped that information from within the TDAmeritrade GUI, and saved it into a CSV.  Now I can load that CSV into\n",
    "#Pandas and save it in a more useful format\n",
    "import pandas as pd\n",
    "file_name = \"Calendar.csv\"\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "#any given news event has an effect on stock prices both before and after the event.  So, we assign a range around the event of\n",
    "#time periods that a given event will be applicable to\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data['datetime'] = data['datetime'].astype('int64')//1e9\n",
    "data['range1'] = data['datetime'] - 60000\n",
    "data['range2'] = data['datetime'] + 60000\n",
    "\n",
    "from collections import defaultdict\n",
    "from joblib import load,dump\n",
    "sdict = defaultdict()\n",
    "for indx in range(len(data)):\n",
    "    s = data['symbol'].iloc[indx]\n",
    "    r1 = data['range1'].iloc[indx]\n",
    "    r2 = data['range2'].iloc[indx]\n",
    "    m = int((r1+r2)/2)\n",
    "    t = data['type'].iloc[indx]\n",
    "    if s not in sdict:\n",
    "        sdict[s] = defaultdict()\n",
    "    sdict[s][(r1,m,r2)] = t\n",
    "    \n",
    "dump(sdict,\"eventdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point I began loading each file of stock data, and saving it in a dataframe with it's relevant news data.\n",
    "#It is at this point that we also compute potentially useful features for experimenting with\n",
    "#there is likely some confusing stuff in here - it was a large project with many challenges along the way.\n",
    "#If someone actually reads through all this and is curious, feel free to email me with your questions and I will\n",
    "#try to answer the best I can\n",
    "\n",
    "import csv \n",
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import *\n",
    "from collections import defaultdict\n",
    "from joblib import load,dump  \n",
    "import os.path\n",
    "from os import path\n",
    "file_name = \"nasdaqtraded.csv\"\n",
    "with open(file_name, 'r') as f: \n",
    "    reader = csv.reader(f)\n",
    "    data = list(list(rec) for rec in csv.reader(f, delimiter=',')) #reads csv into a list of lists\n",
    "\n",
    "stocks = []\n",
    "for l in data:\n",
    "    symb = l[1]\n",
    "    mypath = 'stonks/'+symb+'30'\n",
    "    if not path.exists(mypath):\n",
    "        continue\n",
    "    mypath = \"dataframes2/\"+symb+\"df\"\n",
    "    if path.exists(mypath):\n",
    "        continue        \n",
    "      \n",
    "    stocks.append(symb)\n",
    "    \n",
    "    \n",
    "eventdict = load(\"eventdict\")\n",
    "for symbol in stocks:\n",
    "\n",
    "    if symbol in found:\n",
    "        continue\n",
    "    \n",
    "    newpath = \"dataframes2/\"+symbol+\"df\"\n",
    "    if path.exists(newpath):\n",
    "        found.add(symbol)\n",
    "        continue\n",
    "    stocks30 = load('stonks/'+symbol+'30')    \n",
    "    \n",
    "    if len(stocks30[1]) < 2000:\n",
    "        found.add(symbol)\n",
    "        continue\n",
    "    \n",
    "  \n",
    "    df2 = pd.DataFrame()\n",
    "\n",
    "    df2['vol'] = stocks30[1]\n",
    "    df2['volpct'] = df2['vol'].pct_change()\n",
    "    df2['open'] = stocks30[2]\n",
    "    df2['close'] = stocks30[3]\n",
    "    df2['high'] = stocks30[4]\n",
    "    df2['low'] = stocks30[5]\n",
    "    \n",
    "    elist = [int(x)/1000 for x in stocks30[0]]\n",
    "    df2['epoch'] = [int(x) for x in stocks30[0]]\n",
    "    timelist = [datetime.fromtimestamp(epoch).time() for epoch in elist]\n",
    "    hourlist = [int(str(t)[:2]) for t in timelist]\n",
    "    minutelist = [int(str(t)[3:5]) for t in timelist]\n",
    "    df2['hour'] = hourlist\n",
    "    df2['minute'] = minutelist\n",
    "    \n",
    "    badrows = df2[df2['open'] == 0].index\n",
    "    df2.drop(badrows , inplace=True)\n",
    "    badrows = df2[df2['close'] == 0].index\n",
    "    df2.drop(badrows , inplace=True)\n",
    "    badrows = df2[df2['high'] == 0].index\n",
    "    df2.drop(badrows , inplace=True)\n",
    "    badrows = df2[df2['low'] == 0].index\n",
    "    df2.drop(badrows , inplace=True)   \n",
    "    df2 = df2.replace([np.inf, -np.inf], np.nan).dropna() \n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df2['mean'] = (df2['open']+df2['close'])/2\n",
    "    df2['highdiffclose'] = (df2['high']-df2['close'])/df2['close']\n",
    "    df2['lowdiffclose'] = (df2['close']-df2['low'])/df2['close']\n",
    "    \n",
    "        \n",
    "    #calculate vwap\n",
    "    df2['pv'] = (df2['high']+df2['low']+df2['close'])/3\n",
    "    df2['pv'] = df2['pv']*df2['vol']\n",
    "    df2['voltotal'] = df2['vol'].rolling(200).sum()\n",
    "    df2['pvtotal'] = df2['pv'].rolling(200).sum()\n",
    "    df2['vwap'] = df2['pvtotal']/df2['voltotal']    \n",
    "    del df2['pv']\n",
    "    del df2['voltotal']\n",
    "    del df2['pvtotal']    \n",
    "    \n",
    "    \n",
    "    #calculate RSI\n",
    "    df2['up'] = df2['close'].diff()\n",
    "    df2['down'] = df2['close'].diff()\n",
    "    df2.loc[df2['up'] <= 0, 'up'] = 0\n",
    "    df2.loc[df2['down'] >= 0, 'down'] = 0\n",
    "    df2['up'] = df2['up'].rolling(14).mean()\n",
    "    df2['down'] = df2['down'].rolling(14).mean().abs()\n",
    "    #AvgUt = 1/14 * Ut + 13/14 * AvgUt-1\n",
    "    uplist = list(df2['up'])\n",
    "    downlist = list(df2['down'])\n",
    "    uplist2 = [0]*14\n",
    "    downlist2 = [0]*14\n",
    "    #use wilders\n",
    "    for i in range(14,len(uplist)):\n",
    "        upval = uplist[i]*1/14+uplist[i-1]*13/14\n",
    "        downval = downlist[i]*1/14+downlist[i-1]*13/14\n",
    "        uplist2.append(upval)\n",
    "        downlist2.append(downval)\n",
    "    df2['up'] = uplist2\n",
    "    df2['down'] = downlist2\n",
    "    df2['rs'] = df2['up']/df2['down']\n",
    "    df2['rsi'] = 100 - 100/(1+df2['rs']) \n",
    "    del df2['up']\n",
    "    del df2['down']\n",
    "    del df2['rs']  \n",
    "    \n",
    "    df2['closepct'] = df2['close'].pct_change()\n",
    "    \n",
    "    \n",
    "    #percent change trends\n",
    "    sizelist = [9,30,90,200]\n",
    "    for size in sizelist:\n",
    "        df2['median'+str(size)] = df2['closepct'].rolling(size).median()\n",
    "        df2['pctchange'+str(size)] = df2['close'].pct_change(size)\n",
    "        df2['highest'] = df2['high'].rolling(size).max()\n",
    "        df2['lowest'] = df2['low'].rolling(size).min()\n",
    "        df2['hdc'+str(size)] = (df2['highest']-df2['close'])/df2['close']\n",
    "        df2['ldc'+str(size)] = (df2['close']-df2['lowest'])/df2['close']\n",
    "    \n",
    "    \n",
    "    df2['sma3'] = df2['close'].rolling(3).mean()\n",
    "    df2['sma9'] = df2['close'].rolling(9).mean()\n",
    "    \n",
    "    #find touches, crosses, etc for vwap, sma3, sma9\n",
    "    closeabovevwap = []\n",
    "    closebelowvwap = []\n",
    "    closeabovesma3 = []\n",
    "    closebelowsma3 = []\n",
    "    closeabovesma9 = []\n",
    "    closebelowsma9 = []    \n",
    "    \n",
    "    vwap = list(df2['vwap'])\n",
    "    sma3 = list(df2['sma3'])\n",
    "    sma9 = list(df2['sma9'])\n",
    "    close = list(df2['close']) \n",
    "    high = list(df2['high'])\n",
    "    low = list(df2['low'])\n",
    "    myopen = list(df2['open'])\n",
    "    for indx in range(len(vwap)):\n",
    "        c = close[indx]\n",
    "        s3 = sma3[indx]\n",
    "        s9 = sma9[indx]\n",
    "        v = vwap[indx]\n",
    "        if c > s3:\n",
    "            closeabovesma3.append(1)\n",
    "            closebelowsma3.append(0)\n",
    "        else:\n",
    "            closeabovesma3.append(0)\n",
    "            closebelowsma3.append(1)\n",
    "        if c > s9:\n",
    "            closeabovesma9.append(1)\n",
    "            closebelowsma9.append(0)\n",
    "        else:\n",
    "            closeabovesma9.append(0)\n",
    "            closebelowsma9.append(1)\n",
    "        if c > v:\n",
    "            closeabovevwap.append(1)\n",
    "            closebelowvwap.append(0)\n",
    "        else:\n",
    "            closeabovevwap.append(0)\n",
    "            closebelowvwap.append(1)\n",
    "\n",
    "    df2['closeabovesma3'] = closeabovesma3\n",
    "    df2['closebelowsma3'] = closebelowsma3  \n",
    "    df2['closeabovesma9'] = closeabovesma9  \n",
    "    df2['closebelowsma9'] = closebelowsma9  \n",
    "    df2['closeabovevwap'] = closeabovevwap  \n",
    "    df2['closebelowvwap'] = closebelowvwap  \n",
    "        \n",
    "    #remember, touches mean open and close on same side but high and low not on same side    \n",
    "    tusma3 = []\n",
    "    tdsma3 = []\n",
    "    tusma9 = []\n",
    "    tdsma9 = []\n",
    "    tuvwap = []\n",
    "    tdvwap = []\n",
    "    #remember, crosses means open and close on opp sides\n",
    "    cusma3 = []\n",
    "    cdsma3 = []\n",
    "    cusma9 = []\n",
    "    cdsma9 = []\n",
    "    cuvwap = []\n",
    "    cdvwap = []    \n",
    "    \n",
    "    for i in range(len(df2)):\n",
    "        \n",
    "        vc = vwap[i]\n",
    "        s3c = sma3[i]\n",
    "        s9c = sma9[i]\n",
    "        c = close[i]\n",
    "        h = high[i]\n",
    "        l = low[i]\n",
    "        o = myopen[i]\n",
    "\n",
    "        #vwap crosses and touches\n",
    "        if o <= vc and c > vc:\n",
    "            #cross up\n",
    "            cuvwap.append(1)\n",
    "            cdvwap.append(0)\n",
    "            tuvwap.append(0)\n",
    "            tdvwap.append(0)\n",
    "        elif o <= vc and c <= vc:\n",
    "            cuvwap.append(0)\n",
    "            cdvwap.append(0)\n",
    "            if l <= vc and h > vc:\n",
    "                tuvwap.append(1)\n",
    "                tdvwap.append(0) \n",
    "            else:\n",
    "                tuvwap.append(0)\n",
    "                tdvwap.append(0)\n",
    "        elif o > vc and c <= vc:\n",
    "            #cross down\n",
    "            cuvwap.append(0)\n",
    "            cdvwap.append(1)\n",
    "            tuvwap.append(0)\n",
    "            tdvwap.append(0)                \n",
    "        elif o > vc and c > vc:\n",
    "            cuvwap.append(0)\n",
    "            cdvwap.append(0)\n",
    "            if l <= vc and h > vc:\n",
    "                tuvwap.append(0)\n",
    "                tdvwap.append(1) \n",
    "            else:\n",
    "                tuvwap.append(0)\n",
    "                tdvwap.append(0) \n",
    "        else:\n",
    "            cuvwap.append(0)\n",
    "            cdvwap.append(0)\n",
    "            tuvwap.append(0)\n",
    "            tdvwap.append(0)            \n",
    "        \n",
    "        #sma3 crosses and touches\n",
    "        if o <= s3c and c > s3c:\n",
    "            #cross up\n",
    "            cusma3.append(1)\n",
    "            cdsma3.append(0)\n",
    "            tusma3.append(0)\n",
    "            tdsma3.append(0)\n",
    "        elif o <= s3c and c <= s3c:\n",
    "            cusma3.append(0)\n",
    "            cdsma3.append(0)\n",
    "            if l <= s3c and h > s3c:\n",
    "                tusma3.append(1)\n",
    "                tdsma3.append(0) \n",
    "            else:\n",
    "                tusma3.append(0)\n",
    "                tdsma3.append(0)\n",
    "        elif o > s3c and c <= s3c:\n",
    "            #cross down\n",
    "            cusma3.append(0)\n",
    "            cdsma3.append(1)\n",
    "            tusma3.append(0)\n",
    "            tdsma3.append(0)                \n",
    "        elif o > s3c and c > s3c:\n",
    "            cusma3.append(0)\n",
    "            cdsma3.append(0)\n",
    "            if l <= s3c and h > s3c:\n",
    "                tusma3.append(0)\n",
    "                tdsma3.append(1) \n",
    "            else:\n",
    "                tusma3.append(0)\n",
    "                tdsma3.append(0)   \n",
    "        else:\n",
    "            cusma3.append(0)\n",
    "            cdsma3.append(0)\n",
    "            tusma3.append(0)\n",
    "            tdsma3.append(0)                   \n",
    "        \n",
    "        #sma9 crosses and touches\n",
    "        if o <= s9c and c > s9c:\n",
    "            #cross up\n",
    "            cusma9.append(1)\n",
    "            cdsma9.append(0)\n",
    "            tusma9.append(0)\n",
    "            tdsma9.append(0)\n",
    "        elif o <= s9c and c <= s9c:\n",
    "            cusma9.append(0)\n",
    "            cdsma9.append(0)\n",
    "            if l <= s9c and h > s9c:\n",
    "                tusma9.append(1)\n",
    "                tdsma9.append(0) \n",
    "            else:\n",
    "                tusma9.append(0)\n",
    "                tdsma9.append(0)\n",
    "        elif o > s9c and c <= s9c:\n",
    "            #cross down\n",
    "            cusma9.append(0)\n",
    "            cdsma9.append(1)\n",
    "            tusma9.append(0)\n",
    "            tdsma9.append(0)                \n",
    "        elif o > s9c and c > s9c:\n",
    "            cusma9.append(0)\n",
    "            cdsma9.append(0)\n",
    "            if l <= s9c and h > s9c:\n",
    "                tusma9.append(0)\n",
    "                tdsma9.append(1) \n",
    "            else:\n",
    "                tusma9.append(0)\n",
    "                tdsma9.append(0)             \n",
    "        else:\n",
    "            cusma9.append(0)\n",
    "            cdsma9.append(0)\n",
    "            tusma9.append(0)\n",
    "            tdsma9.append(0)      \n",
    "\n",
    "    \n",
    "    df2['tusma3'] = tusma3\n",
    "    df2['tdsma3'] = tdsma3\n",
    "    df2['tusma9'] = tusma9\n",
    "    df2['tdsma9'] = tdsma9\n",
    "    df2['tuvwap'] = tuvwap\n",
    "    df2['tdvwap'] = tdvwap       \n",
    "    df2['cusma3'] = cusma3\n",
    "    df2['cdsma3'] = cdsma3\n",
    "    df2['cusma9'] = cusma9\n",
    "    df2['cdsma9'] = cdsma9\n",
    "    df2['cuvwap'] = cuvwap\n",
    "    df2['cdvwap'] = cdvwap       \n",
    "\n",
    "    \n",
    "    \n",
    "    #confirmations and bouncebacks\n",
    "    confupsma3 = [0,0]\n",
    "    confdownsma3 = [0,0]\n",
    "    confupsma9 = [0,0]\n",
    "    confdownsma9 = [0,0]\n",
    "    confupvwap = [0,0]\n",
    "    confdownvwap = [0,0]\n",
    "    bbupsma3 = [0,0]\n",
    "    bbdownsma3 = [0,0]\n",
    "    bbupsma9 = [0,0]\n",
    "    bbdownsma9 = [0,0]\n",
    "    bbupvwap = [0,0]\n",
    "    bbdownvwap = [0,0]    \n",
    "    \n",
    "\n",
    "    #now we do bouncebacks and confirmations\n",
    "    for i in range(2,len(df2)):\n",
    "        if (tusma3[i-1] == 1 or tdsma3[i-1] == 1) and (tusma3[i] == 0 and tdsma3[i] == 0 and cusma3[i] == 0 and cdsma3[i] == 0):\n",
    "            confupsma3.append(0)\n",
    "            confdownsma3.append(0)\n",
    "            if close[i] > sma3[i]:\n",
    "                bbupsma3.append(1)\n",
    "                bbdownsma3.append(0)\n",
    "            else:\n",
    "                bbupsma3.append(0)\n",
    "                bbdownsma3.append(1)\n",
    "        else:\n",
    "            bbupsma3.append(0)\n",
    "            bbdownsma3.append(0)\n",
    "            #we can check for confirmation here\n",
    "            if close[i-2] < sma3[i-2] and cusma3[i-1] == 1 and close[i] > sma3[i]:\n",
    "                confupsma3.append(1)\n",
    "                confdownsma3.append(0)\n",
    "            elif close[i-2] > sma3[i-2] and cdsma3[i-1] == 1 and close[i] < sma3[i]:\n",
    "                confupsma3.append(0)\n",
    "                confdownsma3.append(1)\n",
    "            else:\n",
    "                confupsma3.append(0)\n",
    "                confdownsma3.append(0)    \n",
    "                \n",
    "        if (tusma9[i-1] == 1 or tdsma9[i-1] == 1) and (tusma9[i] == 0 and tdsma9[i] == 0 and cusma9[i] == 0 and cdsma9[i] == 0):\n",
    "            confupsma9.append(0)\n",
    "            confdownsma9.append(0)\n",
    "            if close[i] > sma9[i]:\n",
    "                bbupsma9.append(1)\n",
    "                bbdownsma9.append(0)\n",
    "            else:\n",
    "                bbupsma9.append(0)\n",
    "                bbdownsma9.append(1)\n",
    "        else:\n",
    "            bbupsma9.append(0)\n",
    "            bbdownsma9.append(0)\n",
    "            #we can check for confirmation here\n",
    "            if close[i-2] < sma9[i-2] and cusma9[i-1] == 1 and close[i] > sma9[i]:\n",
    "                confupsma9.append(1)\n",
    "                confdownsma9.append(0)\n",
    "            elif close[i-2] > sma9[i-2] and cdsma9[i-1] == 1 and close[i] < sma9[i]:\n",
    "                confupsma9.append(0)\n",
    "                confdownsma9.append(1)\n",
    "            else:\n",
    "                confupsma9.append(0)\n",
    "                confdownsma9.append(0)                   \n",
    "                \n",
    "        if (tuvwap[i-1] == 1 or tdvwap[i-1] == 1) and (tuvwap[i] == 0 and tdvwap[i] == 0 and cuvwap[i] == 0 and cdvwap[i] == 0):\n",
    "            confupvwap.append(0)\n",
    "            confdownvwap.append(0)\n",
    "            if close[i] > vwap[i]:\n",
    "                bbupvwap.append(1)\n",
    "                bbdownvwap.append(0)\n",
    "            else:\n",
    "                bbupvwap.append(0)\n",
    "                bbdownvwap.append(1)\n",
    "        else:\n",
    "            bbupvwap.append(0)\n",
    "            bbdownvwap.append(0)\n",
    "            #we can check for confirmation here\n",
    "            if close[i-2] < vwap[i-2] and cuvwap[i-1] == 1 and close[i] > vwap[i]:\n",
    "                confupvwap.append(1)\n",
    "                confdownvwap.append(0)\n",
    "            elif close[i-2] > vwap[i-2] and cdvwap[i-1] == 1 and close[i] < vwap[i]:\n",
    "                confupvwap.append(0)\n",
    "                confdownvwap.append(1)\n",
    "            else:\n",
    "                confupvwap.append(0)\n",
    "                confdownvwap.append(0)                   \n",
    "        \n",
    "    #now we can place events\n",
    "    earnings = [0]*len(df2)\n",
    "    econoday = [0]*len(df2)\n",
    "    futures = [0]*len(df2)\n",
    "    split = [0]*len(df2)\n",
    "    dividend = [0]*len(df2)\n",
    "    confcall = [0]*len(df2)\n",
    "    \n",
    "    if symbol in eventdict:\n",
    "        curr = eventdict[symbol]\n",
    "        \n",
    "        for i in range(len(df2)):\n",
    "            t = df2['epoch'].iloc[i]/1000\n",
    "            for (r1,m,r2) in list(curr.keys()):\n",
    "                if t > r1 and t < r2:\n",
    "                    mytype = curr[(r1,m,r2)]\n",
    "                    if t >= m:\n",
    "                        if mytype == 'Earnings':\n",
    "                            earnings[i] = 1\n",
    "                        elif mytype == 'Econoday event':\n",
    "                            econoday[i] = 1\n",
    "                        elif mytype == 'Futures Liquidation':\n",
    "                            futures[i] = 1\n",
    "                        elif mytype == 'Split':\n",
    "                            split[i] = 1\n",
    "                        elif mytype == 'Dividend':\n",
    "                            dividend[i] = 1\n",
    "                        elif mytype == 'Conference Call':\n",
    "                            confcall[i] = 1\n",
    "                    else:\n",
    "                        if mytype == 'Earnings':\n",
    "                            earnings[i] = -1\n",
    "                        elif mytype == 'Econoday event':\n",
    "                            econoday[i] = -1\n",
    "                        elif mytype == 'Futures Liquidation':\n",
    "                            futures[i] = -1\n",
    "                        elif mytype == 'Split':\n",
    "                            split[i] = -1\n",
    "                        elif mytype == 'Dividend':\n",
    "                            dividend[i] = -1\n",
    "                        elif mytype == 'Conference Call':\n",
    "                            confcall[i] = -1                \n",
    "      \n",
    "    \n",
    "    targets = list(df2['closepct'].rolling(2).sum())\n",
    "    targets = targets + [0,0]\n",
    "    targets = targets[2:]\n",
    "    df2['targets'] = targets\n",
    "    \n",
    "    #print(len(bbdownvwap),len(df2))\n",
    "    \n",
    "    \n",
    "    df2['confupsma3'] = confupsma3\n",
    "    df2['confdownsma3'] = confdownsma3\n",
    "    df2['confupsma9'] = confupsma9\n",
    "    df2['confdownsma9'] = confdownsma9\n",
    "    df2['confupvwap'] = confupvwap\n",
    "    df2['confdownvwap'] = confdownvwap\n",
    "    df2['bbupsma3'] = bbupsma3\n",
    "    df2['bbdownsma3'] = bbdownsma3\n",
    "    df2['bbupsma9'] = bbupsma9\n",
    "    df2['bbdownsma9'] = bbdownsma9\n",
    "    df2['bbupvwap'] = bbupvwap\n",
    "    df2['bbdownvwap'] = bbdownvwap   \n",
    "    \n",
    "    df2['earnings'] = earnings\n",
    "    df2['econoday'] = econoday\n",
    "    df2['futures'] = futures\n",
    "    df2['split'] = split\n",
    "    df2['dividend'] = dividend\n",
    "    df2['confcall'] = confcall  \n",
    "    \n",
    "    \n",
    "    dump(df2, \"dataframes2/\"+symbol+\"df\")\n",
    "    placed.append(symbol)\n",
    "    \n",
    "    tsize += len(df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I form datasets for my neural network.  I'll make a testing set, and a training set.  \n",
    "#I am limited by memory, but there is no reason a person could not make many training sets and alternate\n",
    "#data between batches.\n",
    "#The datasets are in the form of numpy arrays, since they will be used with keras/tensorflow.\n",
    "#My labels array has three rows, each row corresponds to a different \"prediction range\" - meaning that we\n",
    "#will only want to use one of them at a time.\n",
    "import csv \n",
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import *\n",
    "from collections import defaultdict\n",
    "from joblib import load,dump  \n",
    "import os.path\n",
    "from os import path\n",
    "file_name = \"nasdaqtraded.csv\"\n",
    "with open(file_name, 'r') as f: \n",
    "    reader = csv.reader(f)\n",
    "    data = list(list(rec) for rec in csv.reader(f, delimiter=',')) #reads csv into a list of lists\n",
    "\n",
    "stocks = []\n",
    "for l in data:\n",
    "    symb = l[1]\n",
    "    mypath = \"dataframes2/\"+symb+\"df\"\n",
    "    if not path.exists(mypath):\n",
    "        continue        \n",
    "      \n",
    "    stocks.append(symb)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import random\n",
    "\n",
    "random.shuffle(stocks)\n",
    "\n",
    "pa = np.zeros((60000,500,4)) #closepct,hdc,ldc,vdc,\n",
    "cat = np.zeros((60000,500,5)) #'earnings', 'econoday', 'futures', 'split', 'dividend', 'confcall',\n",
    "time = np.zeros((60000,500,2)) #create on the fly\n",
    "targets = np.zeros((60000,3)) #targets,targets1,targets5\n",
    "\n",
    "for repeat in range(2):\n",
    "\n",
    "snum = 0\n",
    "for symbol in stocks:\n",
    "    if symbol in found:\n",
    "        continue\n",
    "    \n",
    "    df = load(\"dataframes2/\"+symbol+\"df\")\n",
    "    if len(df) < 1000:\n",
    "        found.add(symbol)\n",
    "        continue      \n",
    "          \n",
    "\n",
    "    \n",
    "    arr3 = np.array(df['targets'])[200:-500]\n",
    "    indexes = np.random.choice(np.arange(200,arr3.shape[0]-500), 60, replace=False)\n",
    "        \n",
    "    for z in indexes:\n",
    "        targets[total,0] = np.array(df['targets'])[z+700]\n",
    "        targets[total,1] = np.array(df['targets1'])[z+700]\n",
    "        targets[total,2] = np.array(df['targets5'])[z+700]\n",
    "\n",
    "        pa[total,:,0] = np.array(df['closepct'])[z+200:z+700]\n",
    "        pa[total,:,1] = np.array(df['highdiffclose'])[z+200:z+700]     \n",
    "        pa[total,:,2] = np.array(df['lowdiffclose'])[z+200:z+700]\n",
    "        pa[total,:,3] = np.array(df['vwapdiffclose'])[z+200:z+700]           \n",
    "        cat[total,:,0] = np.array(df['earnings'])[z+200:z+700]\n",
    "        cat[total,:,1] = np.array(df['econoday'])[z+200:z+700]     \n",
    "        cat[total,:,2] = np.array(df['futures'])[z+200:z+700]\n",
    "        cat[total,:,3] = np.array(df['split'])[z+200:z+700]   \n",
    "        cat[total,:,4] = np.array(df['confcall'])[z+200:z+700]     \n",
    "   \n",
    "        hours = list(df['hour'])[z+200:z+700]\n",
    "        minutes = list(df['minute'])[z+200:z+700]\n",
    "        for indx in range(len(hours)):\n",
    "            hour = hours[indx]\n",
    "            minute = minutes[indx]\n",
    "            time[total,indx,0] = hour\n",
    "            time[total,indx,1] = minute\n",
    "        \n",
    "    jj += 1\n",
    "    if snum == 1000:\n",
    "        dump(pa,\"datasets/pa\"+str(repeat))\n",
    "        dump(cat,\"datasets/cat\"+str(repeat))\n",
    "        dump(time,\"datasets/time\"+str(repeat))\n",
    "        dump(targets,\"datasets/target\"+str(repeat))\n",
    "        found.add(symbol)    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can finally get to deep learning.  I used google cloud for training this network, so some of the code may be\n",
    "#specific to using google cloud.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load,dump\n",
    "#!pip3 install git+git://github.com/keras-team/keras.git --upgrade --no-deps\n",
    "from keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, LSTM, Bidirectional, Flatten, Permute, RepeatVector, Multiply, Lambda, Reshape, Cropping1D, Cropping2D, Conv1D, Layer\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.initializers import Ones\n",
    "from keras import backend as K\n",
    "targets = load('target2') #targets,targets1,targets5\n",
    "targets = targets[:,0]\n",
    "targets = targets.reshape((-1,1))\n",
    "pa = load('pa2') #closepct,hdc,ldc,vdc,\n",
    "cat = load('cat2') #'earnings', 'econoday', 'futures', 'split', 'dividend', 'confcall',\n",
    "times = load('time2') #create on the fly\n",
    "\n",
    "\n",
    "#It may seem strange, but I trim my data to 59968 samples.  I did this in an earlier iteration\n",
    "#of the notebook, because I was experimenting with wackier model designs,\n",
    "#and the designs I created were breaking Keras in a way where it couldn't handle\n",
    "#the inputs if they were not an exact multiple of my batch size.\n",
    "time = np.zeros((59968,500,14))\n",
    "#At this point I made my time data sequence a one-hot encoded sequence.  It seems I never actually used an embedding layer for\n",
    "#this, and that's something I may want to try in the future\n",
    "for indx in range(59968):\n",
    "  for subindex in range(500):\n",
    "    hour = int(times[indx,subindex,0])\n",
    "    minute = int(times[indx,subindex,1])\n",
    "    if hour < 12:\n",
    "        time[indx,subindex,0] = 1\n",
    "    else:\n",
    "        hour -= 12\n",
    "        time[indx,subindex,hour] = 1\n",
    "        time[indx,subindex,12] = 1\n",
    "    if minute == 30:\n",
    "        time[indx,subindex,13] = 1\n",
    "\n",
    "del times\n",
    "pa = pa[:59968]\n",
    "time = time[:59968]\n",
    "cat = cat[:59968]\n",
    "labels = labels[:59968]\n",
    "\n",
    "labels = np.zeros((59968,1)) \n",
    "for i in range(59968): #I will use binary crossentropy because the model is more likely to learn if the labels are simple\n",
    "  if targets[i,0] > 0:\n",
    "    labels[i,0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memone = np.ones((59968,1))\n",
    "pa_input = Input(shape=(500,4))\n",
    "cat_input = Input(shape=(500,5))\n",
    "time_input = Input(shape=(500,14))\n",
    "mem_input = Input(shape=(1,))\n",
    "\n",
    "cat_encoding = LSTM(4, activation='relu', return_sequences=True, dropout=0.5)(cat_input)\n",
    "time_encoding = LSTM(4, activation='relu', return_sequences=True, dropout=0.5)(time_input)\n",
    "mainseq = Concatenate()([pa_input, cat_encoding, time_encoding])\n",
    "activations = Bidirectional(LSTM(16, activation='relu', return_sequences=True))(mainseq)\n",
    "attention = Dense(1,activation='tanh')(activations)\n",
    "attention = Flatten()(attention)\n",
    "attention = Dense(500, activation='softmax')(attention)\n",
    "attention = RepeatVector(32)(attention)\n",
    "attention = Permute([2, 1])(attention)\n",
    "result = Multiply()([activations, attention])\n",
    "result = Flatten()(result)\n",
    "result = Dense(256,activation='relu')(result)\n",
    "result = BatchNormalization()(result)\n",
    "\n",
    "last5 = Cropping1D((495,0))(mainseq)\n",
    "last5 = Bidirectional(LSTM(16,activation='relu'))(last5)\n",
    "total = Concatenate()([result,last5])\n",
    "total = Dense(64, activation='relu')(total)\n",
    "context = BatchNormalization()(total)\n",
    "output = Dense(1, activation='sigmoid')(total)\n",
    "  \n",
    "model = Model(inputs=[pa_input,cat_input,time_input,mem_input], outputs=output)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.02, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([pa,cat,time,memone],labels,epochs=25,batch_size=64)\n",
    "\n",
    "#Actual best result on Google Cloud was around 52.5% for both training and validation.  Sounds bad but I think\n",
    "#It's actually pretty decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
